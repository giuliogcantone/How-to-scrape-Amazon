---
title: "Amazon Scraper"
output: github_document
---

This is a guide for R users to automate the copy-paste into a dataset (scrape) of public user-generated content on Amazon. In this example we will scrape the reviews of the users to a jar of italian pesto, but this guide will help you to understand how to efficiently download many kind of public information from websites.

### Packages

I use a very efficient package for installing and loading the necessary packages for scraping, that is `pacman`. In particular, `pacman::p_load(name_of_package_1, name_of_package_2,...)` will check if the packages are installed and, if not, it will install them.

```{r}
pacman::p_load(tidyverse,rvest)
```

# rvest is a great scraper!

To my knowledge, there are essentially 3 free software for scraping things on Internet. We are going to use `rvest`. It is quite easy, intuitive and well-responding. Alternatives are `BeautifulSoup` for Python and Selenium (`RSelenium`). `BeautifulSoup` is conceptually the same of `rvest` so it's only a matter of personal taste, but Selenium and `rvest` works differently.

`rvest` will connect R to the back-end code of any adress where you send it. It always work on crawling the whole code behind a internet page and then your code will select only the user-generated content of interest.

Selenium will set up a bot crawler on a browser that can be guided to perform some actions and literally copy-past elements front-end of the internet page.

In my experience, once setup correctly, Selenium can be even more responsive than `rvest`, because what you (would) see is what you (will) get. This is very useful for scraping websites that relies heavily on Javascripts.

For example, I had to use Selenium to scrape Adults Only content on the website Steam, because to access to the back-code of 18+ content one have before to click on a button of consent. To my knowledge, while `rvest` has a syntax for interacting with elements of front-end, I was never truly able to make it work consistently.

However, I also feel that a correct setup of Selenium is a pain in the ankle to code, and that `rvest` is an excellent beginning. Overall, since Selenium have to synchs to a browser, is also slower than `rvest`.

## rvest main commands

The main commands of rvest are:

* `read_html(URL)`: this commands R to go to a URL and to start watching for the back code of that website addressed at the `URL`.
* `html_elements("selector")`: this is just a filter. To code correctly how to word the `("selector")` is the hard part of any scraping activity. In my opinion, is mostly a work of deduction and reverse engineering.
I think that techniques for deducting the correct selector almost need a guide on their own, but I will try to give my mental tips at the end of this one.
* `html_text()`: this will just convert your selection of elements in a vector of `character`. Sometimes you don't want a `character`, but a `numeric`, but in my opinion having everything scraped as text before pre-processing text will help a lot debugging and it's generally a good practice.

# Scraping pesto!

In this example we want to collect the reviews for a jar of italian pesto. In particular, we will scrape the reviews from Amazon.it, that is the Italian Amazon.

So, we need to set the `URL` of `read_html()` as the address of the webpage for the reviews

```{r}
URL = "https://www.amazon.it/product-reviews/B00XUGZRL8/ref=cm_cr_getr_d_paging_btm_next_2?ie=UTF8&filterByStar=all_stars&reviewerType=all_reviews&pageNumber="

read_html(URL)
```

Now, in theory, we could proceed to select only the part of content of our interest, for example the text of the reviews:

```{r}
read_html(URL) %>%
  html_nodes(".review-text-content span") %>%
  html_text() -> reviews

reviews %>% head(2)
```

There are a bunch of issues to solve, here:

* It only scraped 10 reviews. Why?
* This text has no author, no date, no score...

Let's solve these issue!

## Infer the number of pages to scrape

The scraper downloaded only 10 reviews because in the URL that I provided there are those 10. There are other reviews for the jar of pesto in Amazon, but they are in other URLs. Luckily, it's quite easy to infer where these are...

for example...

```{r}
read_html(URL %>% str_c("2")) %>%
  html_nodes(".review-text-content span") %>%
  html_text() -> reviews

reviews %>% head(2)
```

I only added a number after the URL, now th scraper know that it has to go to page 2 of reviews. Almost all websites are organized exactly in this way. I already pre-coded the URL of Amazon in a way that I only need to add one number to jump into that webpage, and you should try to do the same, understanding the structure of the URL code.

`https://www.amazon.it/product-reviews/B00XUGZRL8/ref=cm_cr_getr_d_paging_btm_next_2?ie=UTF8&filterByStar=all_stars&reviewerType=all_reviews&pageNumber=`

In this case `&pageNumber=` ends the address; this is the reason why the trick with %>% str_c("2") works. You should adapt to the website that you want to scrape and arrange accordingly the address. Usually website have front-ends elements, like buttons or drop-down menus that will help you with it.

So, which is the number of pages for the jar of pesto?

On Amazon it is not explicit, however it is explicit the number of written reviews, that can be selected from the main `URL` with a specific selector.

```{r}
read_html (URL) %>%
  html_node("#filter-info-section span") %>% html_text
```

The information is here, but we need to polish it. The number we are looking for is "290 recensioni globali".

```{r}
read_html (URL) %>%
  html_node("#filter-info-section span") %>% html_text %>% word(25) %>%
  as.integer() -> nrev
nrev
```
Finally, if for each 10 reviews will be generated a new page on Amazon, then the number of pages is...

```{r}
ceiling(nrev / 10) -> lastpage
lastpage
```

## The best way to organize a scraped dataset

Information on timestap, author, text and score goes in parallel, in the sense that there is one author per review, etc.

The ideal data structure to organize parallel observation of different features is the `tibble`, and it is certainly viable to just declare a `tibble` with a fixed number of row and then fill it procedurally with the scraped content.

However, I think that the best method is to fill separate `vectors` and only after the scraping to convert them in a `tibble`. For many `vectors`, the good practice is to organize them in a `list` and then convert the `list` into a `tibble`. I do not like to work with `list` syntax, and `list` are very annoying to browse, but once the coding is done the whole operations are much more smooth.

```{r}
reviews = list()
```

```{r}
for (i in 1:2) {
  
  read_html(URL %>% str_c(i)) -> x
  
reviews$Product[(1+(i-1)*10):(i*10)] = "Pesto Jar"
  
    # Timestamp  
  x %>% html_nodes("#cm_cr-review_list .review-date") %>%
    html_text() -> reviews$Time[(1+(i-1)*10):(i*10)]
  
    # Username  
  x %>% html_nodes("#cm_cr-review_list .a-profile-name") %>%
    html_text() %>%
    ifelse(length(.) > 10,
           rle(.)$values[1:10],
           .) -> reviews$User[(1+(i-1)*10):(i*10)]
  
    # Score
  x %>% html_nodes("#cm_cr-review_list .review-rating") %>%
    html_text() -> reviews$Score[(1+(i-1)*10):(i*10)]
  
    # Comment
  x %>% html_nodes(".review-text-content span") %>%
    html_text() %>% .[. != ""] -> reviews$Text[(1+(i-1)*10):(i*10)]
  
}
```

```{r}
db = as_tibble(reviews)
db
```

